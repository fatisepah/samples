{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMZWLTNLfsv0sygL2XsUxaj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fatisepah/samples/blob/main/Lenet_scratch_with_mul.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "22wlQIiEl0n6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "XBit=3\n",
        "sign=1\n",
        "\n",
        "XYInputBuf=InputBuffer.shape\n",
        "XInputBuf= XYInputBuf[0]\n",
        "YInputBuf= XYInputBuf[1]\n",
        "\n",
        "#find dimention of weight buffer\n",
        "XYWeightBuf=WeightBuffer.shape\n",
        "XWBuf= XYWeightBuf[0]\n",
        "YWBuf= XYWeightBuf[1]\n",
        "lenWeight=len(WeightBuffer)\n",
        "\n",
        "ArrayMulReshape=np.array([])\n",
        "ListKeyWeight=[]\n",
        "DicWeight={}\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#/////////////////////////////////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "def UpDownCounter(Enable,x,IncDec):\n",
        "\tIntX=int(x)\n",
        "\tEnable=int(Enable)\n",
        "\n",
        "\tif Enable != 0:\n",
        "\t\tif IntX == 1:\n",
        "\t\t\tif IncDec == 1:\n",
        "\t\t\t\tIntX=1\n",
        "\t\t\telse:\n",
        "\t\t\t\tIntX=-1\n",
        "\t\telse:\n",
        "\t\t\tIntX=0\n",
        "\telse:\n",
        "\t\tIntX=0\n",
        "\treturn IntX\n",
        "\n",
        "#/////////////////////////////////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "def DownCounter(x):\n",
        " x-=1\n",
        " return x\n",
        "\n",
        "#/////////////////////////////////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "def BISC(In):\n",
        "\t \n",
        "\tLenSc= 2 ** XBit\n",
        "\t\n",
        "\tsc=np.array([])\n",
        "\t\n",
        "\tfor x in range(LenSc):\n",
        "\t\tsc=np.append(sc,[0])\n",
        "\t\t\n",
        "\t\n",
        "\tfor x in range(XBit):\n",
        "\t\ti=(2 ** x)-1\n",
        "\t\twhile i<LenSc:\n",
        "\t\t\tsc[i] = In[x]\n",
        "\t\t\ti+= 2 ** (x+1)\n",
        "\t\t\t\n",
        "\tsc = np.flip(sc)\n",
        "\t# print(\"sc=\",sc)\n",
        "\t\n",
        "\treturn sc\n",
        "\n",
        "#/////////////////////////////////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "def DecimalToBinary(num,XBit):\n",
        "\t\n",
        "\ti = 0\n",
        "\tbnum = []\n",
        "\tBinary=''\n",
        "\twhile num!=0:\n",
        "\t\trem = num%2\n",
        "\t\tbnum.insert(i, rem)\n",
        "\t\ti = i+1\n",
        "\t\tnum = int(num/2)\n",
        "\ti = i-1\n",
        "\twhile i>=0:\n",
        "\t\tBinary=Binary+str(bnum[i])\n",
        "\t\ti = i-1\n",
        "\t\t\n",
        "\twhile len(Binary)<XBit:\n",
        "\t\tBinary='0'+Binary\n",
        "\t\t\n",
        "\t# print(\"Binary:\",Binary)\n",
        "\n",
        "\treturn Binary\n",
        "  \n",
        "#/////////////////////////////////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "def GetDefferentialWArray(w):\n",
        "\tDifferentialWArray=np.array([])\n",
        " \n",
        "\n",
        "\t#sort weight 1-D\n",
        "\tWeightSortArray1D=np.sort(w)\n",
        "\t# print(\"WeightSortArray1D=\",WeightSortArray1D)\n",
        "\t\n",
        "\n",
        "\t#create sorted weight array based on Differential of weights\n",
        "\tDifferentialWArray=np.append(DifferentialWArray,WeightSortArray1D[0])\n",
        "\tfor x in range((XWBuf*YWBuf)-1):\n",
        "\t\tDifferential=WeightSortArray1D[x+1]-WeightSortArray1D[x]\n",
        "\t\tDifferentialWArray=np.append(DifferentialWArray,Differential)\n",
        "\t# print(\"DifferentialWArray=\",DifferentialWArray)\n",
        "\treturn DifferentialWArray\n",
        "\n",
        "#/////////////////////////////////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "def SortDicIndexWeight(w):\n",
        "\n",
        "\tfor x in range(XWBuf):\n",
        "\t\tfor y in range(YWBuf):\n",
        "\t\t\tstr=x,y\n",
        "\t\t\tDicWeight[str]=w[x,y]\n",
        "\t\n",
        "\t#sort dic of weight baes on value:dic(key,value)\n",
        "\tDicWSort=dict(sorted(DicWeight.items(), key=lambda item: item[1]))\n",
        "\t# print(\"DicWSort=\",DicWSort)\n",
        "\n",
        "\tfor x in range(XWBuf*YWBuf):\n",
        "\t\tListKeyWeight.append(list(DicWSort.keys())[x])\n",
        "\t# print(\"ListKeyWeight=\",ListKeyWeight)\n",
        "\n",
        "#/////////////////////////////////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "\n",
        "#function for doing the functional of PE in skippy based on BISC & Uo/Down counter & Down counter\n",
        "def Mul(InputBuffer,WeightBuffer):\n",
        "\tOutputArray=np.array([])\n",
        "\n",
        "\t#reshape WeightSortArray to 1-D\n",
        "\tWeightBuffer1D=WeightBuffer.reshape(-1)\n",
        "\n",
        "\tDifferentialW=GetDefferentialWArray(WeightBuffer1D)\n",
        "\n",
        "\tClockNum=0\n",
        "\tfor i in range(len(DifferentialW)):\n",
        "\t\tClockNum=ClockNum+abs(DifferentialW[i])\n",
        "\n",
        "\tClockNum*=((XInputBuf-XWBuf)+1)*((YInputBuf-YWBuf)+1)\n",
        "\t# print(\"ClockNum=\",ClockNum)\n",
        "\n",
        "\t\n",
        "\n",
        "\n",
        "\tSortDicIndexWeight(WeightBuffer)\n",
        "\n",
        "\t\n",
        "\tfor x in range((XInputBuf-XWBuf)+1):\n",
        "\t\tfor y in range((YInputBuf-YWBuf)+1):\n",
        "\t\t\t\n",
        "\t\t\t\n",
        "\t\t\tSelectIn=InputBuffer[x:XWBuf+x ,y:YWBuf+y]\n",
        "\t\t\t\n",
        "\t\t\tprint(\"NewArray=\",SelectIn)\n",
        "\t\t\t\n",
        "\t\t\tSumOut=0\n",
        "\t\t\tfor a in range(XWBuf):\n",
        "\t\t\t\tfor b in range(YWBuf):\n",
        "\t\t\t\t\t\n",
        "\t\t\t\t\tLenDiffW= len(DifferentialW)\n",
        "\t\t\t\t\tSumList=[]\n",
        "\t\t\t\t\tfor s in range(LenDiffW):\n",
        "\t\t\t\t\t\tSumList.append(0)\n",
        "\t\t\t\t\t\t\n",
        "\t\t\t\t\tX=SelectIn[a,b]\n",
        "\t\t\t\t\tXBinary=DecimalToBinary(X,XBit)\n",
        "\n",
        "\t\t\t\t\t\n",
        "\t\t\t\t\tSC=BISC(XBinary)\n",
        "\n",
        "\t\t\t\t\tandisY=0\n",
        "\t\t\t\t\tfor z in range(LenDiffW):\n",
        "\t\t\t\t\t\tandisSC=0\n",
        "\t\t\t\t\t\t\n",
        "\t\t\t\t\t\tValWInDiff=DifferentialW[z]\n",
        "\t\t\t\t\t\t#print(type(ValWInDiff))\n",
        "        \n",
        "\n",
        "\n",
        "\t\t\t\t\t\tif ValWInDiff == 0:\n",
        "\t\t\t\t\t\t\tif z==0:\n",
        "\t\t\t\t\t\t\t\tSumList[andisY]=0\n",
        "\t\t\t\t\t\t\telse:\n",
        "\t\t\t\t\t\t\t\tSumList[andisY]=SumList[andisY-1]\n",
        "\t\t\t\t\t\t\tandisY+=1\n",
        "\t\t\t\t\t\t\tcontinue\t\t\t\t\t\t\n",
        "\t\t\t\t\t\t\n",
        "\t        \t\t\t#instantiating the variable of sign holder \n",
        "\t\t\t\t\t\t#if sign=0, number is negative & sign=1, number is positive\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\n",
        "\t\t\t\t\t\tif ValWInDiff<0:\n",
        "\t\t\t\t\t\t\tsign=0\n",
        "\t\t\t\t\t\t\tValWInDiff=abs(ValWInDiff)\n",
        "\t\t\t\t\t\telse:\n",
        "\t\t\t\t\t\t\tsign=1\n",
        "\t\t\t\t\t\t\t\n",
        "\n",
        "\t\t\t\t\t\t\n",
        "\t\t\t\t\t\tsum=0\n",
        "\t\t\t\t\t\twhile ValWInDiff>0:\n",
        "\t\t\t\t\t\t\t\n",
        "\t\t\t\t\t\t\tOutUDCounter=UpDownCounter(ValWInDiff,SC[(len(SC)-1)-andisSC],sign)\n",
        "\t\t\t\t\t\t\tandisSC+=1\n",
        "\t\t\t\t\t\t\tValWInDiff=DownCounter(ValWInDiff)\n",
        "\t\t\t\t\t\t\tsum=sum+OutUDCounter\n",
        "\t\t\t\t\t\t\t\n",
        "\t\t\t\t\t\tif z==0:\n",
        "\t\t\t\t\t\t\tSumList[andisY]=sum\n",
        "\t\t\t\t\t\telse:\n",
        "\t\t\t\t\t\t\tSumList[andisY]=SumList[andisY-1]+sum\n",
        "\t\t\t\t\t\tandisY+=1\n",
        "\n",
        "\t\t\t\t\t# print(\"SumList=\",SumList)\n",
        "\n",
        "\t\t\t\t\tIndex=ListKeyWeight.index((a,b))\n",
        "\t\t\t\t\tval=SumList[Index]\n",
        "\n",
        "\t\t\t\t\t# print(\"ValIndex=\",val)\t\n",
        "\t\t\t\t\tSumOut=SumOut+val\n",
        "\t\t\t\t\tif SumOut >= (2**XBit):\n",
        "\t\t\t\t\t\tSumOut=2**XBit\n",
        "\t\t\t\t\t\tbreak\n",
        "\t\t\t\t\tif SumOut <= -(2**XBit):\n",
        "\t\t\t\t\t\tSumOut=-(2**XBit)\n",
        "\t\t\t\t\t\tbreak\n",
        "\t\t\tprint(\"SumOut=\",SumOut)\n",
        "\n",
        "\t\t\t#two lines for compute the last result baesd on stochastic(divide on 2**XBit)\n",
        "\t\t\tSumOut=SumOut/2**XBit\n",
        "\t\t\tprint(\"SumOut=\",SumOut)\t\n",
        "\n",
        "\t\t\tOutputArray=np.append(OutputArray,SumOut)\n",
        "\t\t\t\n",
        "\n",
        "\tprint(\"OutputArray=\",OutputArray)\n",
        "\tNewOutputArray=OutputArray.reshape((XInputBuf-XWBuf)+1,(YInputBuf-YWBuf)+1)\n",
        "\tprint(\"NewOutputArray=\",NewOutputArray)\n",
        "\treturn NewOutputArray\n"
      ],
      "metadata": {
        "id": "WkskkNa53D8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Convolution2D:\n",
        "\n",
        "    def __init__(self, inputs_channel, num_filters, kernel_size, padding, stride):\n",
        "        # weight size: (F, C, K, K)\n",
        "        # bias size: (F) \n",
        "        self.F = num_filters\n",
        "        self.K = kernel_size\n",
        "        self.C = inputs_channel\n",
        "\n",
        "        self.weights = np.zeros((self.F, self.C, self.K, self.K))\n",
        "        self.bias = np.zeros((self.F, 1))\n",
        "        for i in range(0,self.F):\n",
        "            self.weights[i,:,:,:] = np.random.normal(loc=0, scale=np.sqrt(1./(self.C*self.K*self.K)), size=(self.C, self.K, self.K))\n",
        "\n",
        "        self.p = padding\n",
        "        self.s = stride\n",
        "\n",
        "        # print(\"F(num_filters)ConvLayer=\",self.F)\n",
        "        # print(\"K(kernel_size)ConvLayer=\",self.K)\n",
        "        # print(\"C(inputs_channel)ConvLayer=\",self.C)\n",
        "\n",
        "    def zero_padding(self, inputs, size):\n",
        "        w, h = inputs.shape[0], inputs.shape[1]\n",
        "        new_w = 2 * size + w\n",
        "        new_h = 2 * size + h\n",
        "        out = np.zeros((new_w, new_h))\n",
        "        out[size:w+size, size:h+size] = inputs\n",
        "        \n",
        "        return out\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # input size: (C, W, H)\n",
        "        # output size: (N, F ,WW, HH)\n",
        "        # print(\"inputOfConvLayer=\",inputs)\n",
        "        # print(\"inputShapeOfConvLayer=\",inputs.shape)\n",
        "\n",
        "        C = inputs.shape[0]\n",
        "        # print(\"C-inputs.shape[0]=\",C)\n",
        "\n",
        "        W = inputs.shape[1]+2*self.p\n",
        "        # print(\"W-inputs.shape[1]=\",W)\n",
        "\n",
        "        H = inputs.shape[2]+2*self.p\n",
        "        # print(\"H-inputs.shape[2]=\",H)\n",
        "\n",
        "        self.inputs = np.zeros((C, W, H))\n",
        "        # print(\"self.inputsConv=\",self.inputs)\n",
        "\n",
        "        for c in range(inputs.shape[0]):\n",
        "            self.inputs[c,:,:] = self.zero_padding(inputs[c,:,:], self.p)\n",
        "        # print(\"OutZero_padding=\",self.inputs)\n",
        "\n",
        "        #############  \n",
        "        WW =int((W - self.K)/self.s + 1)\n",
        "        # print(\"WW=\",WW)\n",
        "        ##########\n",
        "        HH = int((H - self.K)/self.s + 1)\n",
        "        # print(\"HH=\",HH)\n",
        "\n",
        "        feature_maps = np.zeros((self.F, WW, HH))\n",
        "        # print(\"feature_mapsOfConvLayerzero=\",feature_maps)\n",
        "\n",
        "        for f in range(self.F):\n",
        "            for w in range(0, WW, self.s):\n",
        "                for h in range(0, HH, self.s):\n",
        "                    # print(\"SelectInputh=\",self.inputs[:,w:w+self.K,h:h+self.K])\n",
        "                    sel=self.inputs[:,w:w+self.K,h:h+self.K]\n",
        "                    # print(\"SelectInputShape=\",sel.shape)\n",
        "                    Weights=self.weights[f,:,:,:]\n",
        "                    # print(\"WeightShape=\",Weights.shape)\n",
        "                    feature_maps[f,w,h]=np.sum(self.inputs[:,w:w+self.K,h:h+self.K]*self.weights[f,:,:,:])+self.bias[f]\n",
        "                    # print(\"feature_mapsOfConvLayer=\",feature_maps)\n",
        "        # print(\"Lastfeature_mapsOfConvLayer=\",feature_maps.shape)\n",
        "        # print(\"conv's forward is done\")\n",
        "\n",
        "        return feature_maps\n",
        "    \n",
        "\n",
        "    def backward(self, output_error, learning_rate):\n",
        "        # print(\"output_errorInBackConvShape=\",output_error.shape)\n",
        "\n",
        "        C, W, H = self.inputs.shape\n",
        "        # print(\"inputsconvvvv.shape=\",self.inputs.shape)\n",
        "\n",
        "        WW =int((W - self.K)/self.s + 1)\n",
        "        # print(\"WW=\",WW)\n",
        "        ##########\n",
        "        HH = int((H - self.K)/self.s + 1)\n",
        "        # print(\"HH=\",HH)\n",
        "\n",
        "\n",
        "        dx = np.zeros(self.inputs.shape)\n",
        "        dw = np.zeros(self.weights.shape)\n",
        "        db = np.zeros(self.bias.shape)\n",
        "\n",
        "\n",
        "        F= output_error.shape[0]\n",
        "        # print(\"F=\",F,\"ForOutputErrorShapeInBackConv\")\n",
        "        output_error=np.reshape(output_error,(F,WW,HH))\n",
        "        \n",
        "        # print(\"output_errorInBackConvShape=\",output_error.shape)\n",
        "\n",
        "        for f in range(F):\n",
        "            for w in range(0, WW, self.s):\n",
        "                for h in range(0, HH, self.s):\n",
        "                    dw[f,:,:,:]+=output_error[f,w,h]*self.inputs[:,w:w+self.K,h:h+self.K]\n",
        "                    dx[:,w:w+self.K,h:h+self.K]+=output_error[f,w,h]*self.weights[f,:,:,:]\n",
        "\n",
        "        for f in range(F):\n",
        "            db[f] = np.sum(output_error[f, :, :])\n",
        "\n",
        "        self.weights -= learning_rate * dw\n",
        "        self.bias -= learning_rate * db\n",
        "        # print(\"conv's backward is done\")\n",
        "        return dx\n",
        "\n",
        "    def extract(self):\n",
        "        return {self.name+'.weights':self.weights, self.name+'.bias':self.bias}\n",
        "\n",
        "    def feed(self, weights, bias):\n",
        "        self.weights = weights\n",
        "        self.bias = bias"
      ],
      "metadata": {
        "id": "Z9YCSRwH0MbL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Maxpooling2D:\n",
        "\n",
        "    def __init__(self, pool_size, stride):\n",
        "        self.pool = pool_size\n",
        "        self.s = stride\n",
        "        # print(\"pool_size(pooling layer)=\",self.pool)\n",
        "        # print(\"self.s(pooling layer)=\",self.s)\n",
        "\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        self.inputs = inputs\n",
        "        C, W, H = inputs.shape\n",
        "        # print(\"InputsShape(pooling layer)=\",inputs.shape)\n",
        "        new_width =int((W - self.pool)/self.s + 1)\n",
        "        # print(\"new_width(pooling layer)=\",new_width)\n",
        "        new_height = int((H - self.pool)/self.s + 1)\n",
        "        # print(\"new_height(pooling layer)=\",new_height)\n",
        "        out = np.zeros((C, new_width, new_height))\n",
        "        for c in range(C):\n",
        "            for w in range(int(W/self.s)):\n",
        "                for h in range(int(H/self.s)):\n",
        "                    out[c, w, h] = np.max(self.inputs[c, w*self.s:w*self.s+self.pool, h*self.s:h*self.s+self.pool])\n",
        "        # print(\"pool's forward is done\")\n",
        "        return out\n",
        "\n",
        "    def backward(self,  output_error, learning_rate):\n",
        "        # print(\"output_errorInBackMax=\",output_error.shape)\n",
        "        C, W, H = self.inputs.shape\n",
        "        # print(\"C=\",C,\"W=\",W,\"H=\",H,\"ForOutputErrorShapeInBackMax\")\n",
        "        dx = np.zeros(self.inputs.shape)\n",
        "        \n",
        "        for c in range(C):\n",
        "            for w in range(0, W, self.pool):\n",
        "                for h in range(0, H, self.pool):\n",
        "                    # print(\"selectInput=\",self.inputs[c,w:w+self.pool,h:h+self.pool])\n",
        "                    # print(\"argmaxSelectInput=\",np.argmax(self.inputs[c,w:w+self.pool,h:h+self.pool]))\n",
        "                    st = np.argmax(self.inputs[c,w:w+self.pool,h:h+self.pool])\n",
        "                    # print(\"stShape=\",st.shape)\n",
        "                    (idx, idy) = np.unravel_index(st, (self.pool, self.pool))\n",
        "                    # print(\"idx=\",idx,\"idy=\",idy)\n",
        "                    dx[c, w+idx, h+idy] = output_error[c,int( w/self.pool), int(h/self.pool)]\n",
        "                    # print(\"dxshape=\",dx.shape)\n",
        "        # print(\"pool's backward is done\")\n",
        "        return dx\n",
        "\n",
        "    def extract(self):\n",
        "        return "
      ],
      "metadata": {
        "id": "uhZYbVwxtd7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTrTMpTwtLXd"
      },
      "source": [
        "class FCLayer:\n",
        "    def __init__(self, input_size, output_size):\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        # print(\"input_size(FC layer)=\",self.input_size)\n",
        "        # print(\"output_size(FC layer)=\",self.output_size)\n",
        "        self.weights = np.random.randn(input_size, output_size) / np.sqrt(input_size + output_size)\n",
        "        self.bias = np.random.randn(1, output_size) / np.sqrt(input_size + output_size)\n",
        "\n",
        "    def forward(self, input):\n",
        "        self.input = input\n",
        "        return np.dot(input, self.weights) + self.bias\n",
        "        # print(\"FC's forward is done\")\n",
        "\n",
        "    def backward(self, output_error, learning_rate):\n",
        "        # print(\"FCLayer's shape_output_error----In=\",output_error.shape)\n",
        "        input_error = np.dot(output_error, self.weights.T)\n",
        "        weights_error = np.dot(self.input.T, output_error)\n",
        "        # bias_error = output_error\n",
        "        \n",
        "        \n",
        "        self.weights -= learning_rate * weights_error\n",
        "        self.bias -= learning_rate * output_error\n",
        "        # print(\"FCLayer input_errorShape=\",input_error.shape)\n",
        "        # print(\"FC's backward is done\")\n",
        "        return input_error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6nSYAB2sam3"
      },
      "source": [
        "class ActivationLayer:\n",
        "    def __init__(self, activation, activation_prime):\n",
        "        self.activation = activation\n",
        "        self.activation_prime = activation_prime\n",
        "    \n",
        "    def forward(self, input):\n",
        "        self.input = input\n",
        "        return self.activation(input)\n",
        "        # print(\"Activation's forward is done\")\n",
        "    \n",
        "    def backward(self, output_error, learning_rate):\n",
        "        return output_error * self.activation_prime(self.input)\n",
        "        # print(\"Activation's backward is done\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hl8LxP1lAEiN"
      },
      "source": [
        "# bonus\n",
        "class FlattenLayer:\n",
        "    def __init__(self, input_shape):\n",
        "        self.input_shape = input_shape\n",
        "        # print(\"input_shape(Flatten layer)=\",self.input_shape)\n",
        "\n",
        "\n",
        "    def forward(self, input):\n",
        "        return np.reshape(input, (1, -1))\n",
        "        # print(\"Flatten's forward is done\")\n",
        "    \n",
        "    def backward(self, output_error, learning_rate):\n",
        "        # print(\"Flatten's shape_output----In=\",output_error.shape)\n",
        "        # print(\"Flatten's shape_output\",(np.reshape(output_error, self.input_shape)).shape)\n",
        "        return np.reshape(output_error, self.input_shape)\n",
        "        # print(\"Flatten's backward is done\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Flatten:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    def forward(self, inputs):\n",
        "        self.C, self.W, self.H = inputs.shape\n",
        "        return inputs.reshape(1, self.C*self.W*self.H)\n",
        "    def backward(self, dy):\n",
        "        return dy.reshape(self.C, self.W, self.H)\n",
        "    def extract(self):\n",
        "        return"
      ],
      "metadata": {
        "id": "PHs8z8hNyb49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQeuIfkK3vyl"
      },
      "source": [
        "# bonus\n",
        "class SoftmaxLayer:\n",
        "    def __init__(self, input_size):\n",
        "        self.input_size = input_size\n",
        "        # print(\"input_size(Softmax layer)=\",self.input_size)\n",
        "    \n",
        "    def forward(self, input):\n",
        "        self.input = input\n",
        "        tmp = np.exp(input)\n",
        "        self.output = tmp / np.sum(tmp)\n",
        "        # print(\"Softmax's forward is done\")\n",
        "        return self.output\n",
        "    \n",
        "    def backward(self, output_error, learning_rate):\n",
        "        # print(\"SoftmaxLayer's shape_output----In=\",output_error.shape)\n",
        "        input_error = np.zeros(output_error.shape)\n",
        "        out = np.tile(self.output.T, self.input_size)\n",
        "        # print(\"Softmax's backward is done\")\n",
        "        return self.output * np.dot(output_error, np.identity(self.input_size) - out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LuPbn70Wt8Q7"
      },
      "source": [
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_prime(x):\n",
        "    return np.exp(-x) / (1 + np.exp(-x))**2\n",
        "\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "def tanh_prime(x):\n",
        "    return 1 - np.tanh(x)**2\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(x, 0)\n",
        "\n",
        "def relu_prime(x):\n",
        "    return np.array(x >= 0).astype('int')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXY7jkUzuqEk"
      },
      "source": [
        "def mse(y_true, y_pred):\n",
        "    return np.mean(np.power(y_true - y_pred, 2))\n",
        "\n",
        "def mse_prime(y_true, y_pred):\n",
        "    return 2 * (y_pred - y_true) / y_pred.size\n",
        "\n",
        "def sse(y_true, y_pred):\n",
        "    return 0.5 * np.sum(np.power(y_true - y_pred, 2))\n",
        "\n",
        "def sse_prime(y_true, y_pred):\n",
        "    return y_pred - y_true"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-whGNp8Joaur",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d9364d2-b3d3-4502-f0eb-f1a1a5efd9b4"
      },
      "source": [
        "from keras.datasets import mnist\n",
        "from keras.utils import np_utils\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_train /= 255\n",
        "y_train = np_utils.to_categorical(y_train)\n",
        "x_train = x_train[0:1000]\n",
        "y_train = y_train[0:1000]\n",
        "\n",
        "x_test = x_test.astype('float32')\n",
        "x_test /= 255\n",
        "y_test = np_utils.to_categorical(y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHQpwN8LpKiN"
      },
      "source": [
        "# unlike the Medium article, I am not encapsulating this process in a separate class\n",
        "# I think it is nice just like this\n",
        "network = [\n",
        "    Convolution2D(1, 6, 5, 2, 1),\n",
        "    ActivationLayer(relu, relu_prime),\n",
        "    Maxpooling2D(2,2),\n",
        "    Convolution2D(6,16,5,0,1),\n",
        "    ActivationLayer(relu, relu_prime),\n",
        "    Maxpooling2D(2,2),\n",
        "    Convolution2D(16, 120,5,0,1),\n",
        "    FlattenLayer(120),\n",
        "    # FCLayer(400, 120),\n",
        "    FCLayer(120, 84),\n",
        "    ActivationLayer(relu, relu_prime),\n",
        "    FCLayer(84, 10),\n",
        "    SoftmaxLayer(10)\n",
        "]\n",
        "\n",
        "epochs = 40\n",
        "learning_rate = 0.1\n",
        "\n",
        "# training\n",
        "for epoch in range(epochs):\n",
        "    error = 0\n",
        "    for x, y_true in zip(x_train, y_train):\n",
        "        # forward\n",
        "        # print(\"x=\",x)\n",
        "        # print(\"y_true=\",y_true)\n",
        "        # print(\"shapex=\",x.shape)\n",
        "        output = x.reshape((1,x.shape[0], x.shape[1]))\n",
        "        # print(\"xReshape=\",output)\n",
        "        # print(\"shapexReshape=\",output.shape)\n",
        "        for layer in network:\n",
        "            output = layer.forward(output)\n",
        "            # print(\"forward\",layer, \"finish\")\n",
        "        \n",
        "        # print(\"forward epoch\",epoch,\"finish\")\n",
        "        # error (display purpose only)\n",
        "        error += mse(y_true, output)\n",
        "\n",
        "        # backward\n",
        "        output_error = mse_prime(y_true, output)\n",
        "\n",
        "        for layer in reversed(network):\n",
        "            # print(\"backward\",layer,\"finish\")\n",
        "            output_error = layer.backward(output_error, learning_rate)\n",
        "\n",
        "            # print(\"output_errorShape=\",output_error.shape)\n",
        "            \n",
        "\n",
        "        # print(\"backward epoch\",epoch,\"finish\")\n",
        "    \n",
        "    error /= len(x_train)\n",
        "    print('%d/%d, error=%f' % (epoch + 1, epochs, error))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrMLx3eGv3jk"
      },
      "source": [
        "def predict(network, input):\n",
        "    \n",
        "    x=input.reshape(1,input.shape[0],input.shape[1])\n",
        "    # print(\"shapereshape\",(input.reshape(1,input.shape[0],input.shape[1])).shape)\n",
        "    output = x\n",
        "    # print(\"inputShape\",input.shape)\n",
        "    for layer in network:\n",
        "        \n",
        "        output = layer.forward(output)\n",
        "        # print(\"outputShape\",output.shape)\n",
        "    return output\n",
        "\n",
        "ratio = sum([np.argmax(y) == np.argmax(predict(network, x)) for x, y in zip(x_test, y_test)]) / len(x_test)\n",
        "error = sum([mse(y, predict(network, x)) for x, y in zip(x_test, y_test)]) / len(x_test)\n",
        "print('ratio: %.2f' % ratio)\n",
        "print('mse: %.4f' % error)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERV5_QinvwXY"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "samples = 10\n",
        "for test, true in zip(x_test[:samples], y_test[:samples]):\n",
        "    image = np.reshape(test, (28, 28))\n",
        "    plt.imshow(image, cmap='binary')\n",
        "    plt.show()\n",
        "    pred = predict(network, test)[0]\n",
        "    idx = np.argmax(pred)\n",
        "    idx_true = np.argmax(true)\n",
        "    print('pred: %s, prob: %.2f, true: %d' % (idx, pred[idx], idx_true))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}