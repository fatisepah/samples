{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fatisepah/samples/blob/CNN-from-scratch/Lenet5_for_conv's_mul_using_stochastic_scratch_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npFemIWusYQi"
      },
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "from numpy import array \n",
        "from numpy import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "XBit=8\n",
        "sign=1\n",
        "print(\"XBit:\",XBit)\n",
        "\n",
        "InputBuffer = random.randint(256, size=(28, 28))\n",
        "WeightBuffer = random.randint(8, size=(9, 9))\n",
        "\n",
        "print(\"InputBuffer=\",InputBuffer)\n",
        "\n",
        "print(\"WeightBuffer=\",WeightBuffer)\n",
        "\n",
        "#find dimention of input buffer\n",
        "XYInputBuf=InputBuffer.shape\n",
        "XInputBuf= XYInputBuf[0]\n",
        "YInputBuf= XYInputBuf[1]\n",
        "\n",
        "#find dimention of weight buffer\n",
        "XYWeightBuf=WeightBuffer.shape\n",
        "XWBuf= XYWeightBuf[0]\n",
        "YWBuf= XYWeightBuf[1]\n",
        "lenWeight=len(WeightBuffer)\n",
        "\n",
        "ArrayMulReshape=np.array([])\n",
        "ListKeyWeight=[]\n",
        "DicWeight={}\n"
      ],
      "metadata": {
        "id": "WnctKj5T9RvQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c6b6689-efc4-425b-e82f-7c18bafa4590"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XBit: 8\n",
            "InputBuffer= [[188  33  38 162 138 140 149 145  16  74  54 113  11  71 115  46  79 141\n",
            "  109 129 174  26 137 226 122  94 129 106]\n",
            " [237 236 125  52 187 226  76 125  55 251   7 232  67 142 109  32  78 164\n",
            "   30 232 163 176 150 211 192  84 184  50]\n",
            " [143  35 234 241 222  26 202  69 199  55  27 135  99  59  97  78 211  74\n",
            "  224 206  99 192  16 213  53  11  27 173]\n",
            " [ 77  80   4 235 136 211  25 103  43  53 148 220  83  36 212 206  29 217\n",
            "   99 235 249 215 234 106  64 164  29  14]\n",
            " [ 24 191  26  60  41 194 115  37   1 197   5 222  55 222 110 122 185 153\n",
            "   82 219 136   4  13  86  57 160 242 152]\n",
            " [219 197 213 168 206   2 122  93  15 112 173  51  17 191  14  86  94   2\n",
            "  168  92 156 242 244 225 160 134   9 183]\n",
            " [ 91 136 157 138 137 168 229 194  82  22 156  15 191 104 181 223 112  69\n",
            "  200 255  91 203 155  45 162 161 217 143]\n",
            " [ 73  16 216 180 186 217  45 158 153 132 213  85 149  49 112 232 107  82\n",
            "  194 102 176 120  42  80 100  34   6 170]\n",
            " [134  55  19 141 130  69 135 106 234  51 102 192  24 151   6  32 195  30\n",
            "   30 202  47  28  18 199 142 205 178 111]\n",
            " [143 141  54  77 119 246  87  97  14 102 234  80 140   0   1  32  65  93\n",
            "  244 217 115  11 237 131 234  11  33 184]\n",
            " [232  86 192  90 236 157 124 243  50  48 216 216 100 194  31  55 166 201\n",
            "   54  68  50 245  19  96  80 181 186 174]\n",
            " [  7 142 198  27 219  71  43  10 255 237  46 116  15 220  43 198 147  91\n",
            "  244 184 169 211 182 145  89 212  42 143]\n",
            " [ 86 216 153 188  46  82  98 253  27  88 242  80 141   1 191 176 119  59\n",
            "  115  69 183  36 163 231  68 247  53 180]\n",
            " [ 90  24 197 164  53 227 139 103 193   4 211  12 110 254  21 112 117 181\n",
            "   88 141 253 130 170  48   9 227  77  37]\n",
            " [203   8  38  65 154 225 224 220  48  27 148  41  26  97  20 238 159 252\n",
            "  214 119  20  27 105  15 196 100 254 253]\n",
            " [177 245  81 251 117   4 118 186 205   0  49  31  61  77  89 249 210  44\n",
            "  139 254 153 174 160 139 249 166 135 204]\n",
            " [ 43  38  73 185  41 226  69 114  50 169 238  77 204 245  83 244  89 144\n",
            "   37 215 149 137  56  55  20  15  83 181]\n",
            " [252 240  68  88 114  36 133 169  77 146 106  46  74 187 161 138 102 163\n",
            "   97 178 219 251 135  16 124  82  61  60]\n",
            " [237 233  51  58  68  71 218 214 246  14 209 150 106 135 236  90 223  32\n",
            "  187 203 200  63  28   9  65 205 199  66]\n",
            " [229  52 207  23  67 120 137 124 156  10 209  73  42  10  24 106  43 193\n",
            "  144  83  80  18 204 240  89  94  78 219]\n",
            " [ 47   0  42 111   7  32 105  38  23  27 131  93  96  81 137   7 245 100\n",
            "  233 124 190 134 216  21 251 174  81 124]\n",
            " [194  51 148 174  58  32  64 226 134 128  33 207 174 239  82 161 140 236\n",
            "  132 253  40 179 242 204 120 146 137 168]\n",
            " [233  31 125  22 196 234 108 155 227  45  55 233 209 202  56   2 140 184\n",
            "  169 114   6  88  66  38 175  42  27  52]\n",
            " [234 220  16 142  86 102 241 241  26 185  42 209 168  48  26  36  94 141\n",
            "  224  26 151  57  21 162  51 134  28  83]\n",
            " [246  43  48 125 211 121 244 135 122  96 224  25 200 117  61  52 144 246\n",
            "  223 201  74 209  26 100 241  27 123 248]\n",
            " [218 121  20  60  48 252 102  43 254  86 130  20 105 127 210 170 162  86\n",
            "  169  38 171 142  58   9  11 107  68  75]\n",
            " [ 33 232 180 202  32 217 227 137 239  33 151 171 190 252 137  45 159   2\n",
            "   97 194 106 224 158 110  35 194 158 118]\n",
            " [ 35  32  15  20  98  18 176  16  95  40 225 194 181 207 201 159   3  50\n",
            "   96  30 206  36 112 164  23  60 170   1]]\n",
            "WeightBuffer= [[4 6 3 4 6 4 2 3 1]\n",
            " [0 1 6 0 0 4 3 2 7]\n",
            " [5 0 7 0 0 6 3 4 5]\n",
            " [0 4 4 3 0 0 6 4 6]\n",
            " [1 3 0 7 0 2 1 0 6]\n",
            " [6 0 4 5 1 6 4 1 5]\n",
            " [3 7 4 3 6 7 6 0 5]\n",
            " [7 1 0 0 6 2 0 3 7]\n",
            " [0 7 2 7 5 0 3 7 4]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def DetectTheFirstValueOfRange(WeightBuffer):\n",
        "\n",
        "\t#reshape WeightSortArray to 1-D\n",
        "\tWeightBuffer1D=WeightBuffer.reshape(-1)\n",
        "\tprint(\"WeightBuffer1D=\",WeightBuffer1D)\n",
        "\tXWeightBuf1D=WeightBuffer1D.shape[0]\n",
        "\n",
        "\n",
        "    #changing value of weightBuffer with the first value of each range\n",
        "\ti=0\n",
        "\twhile i<XWeightBuf1D:\n",
        "\t\tif WeightBuffer1D[i]<0:\n",
        "\t\t\tfor x in range(4):\n",
        "\t\t\t\tNfirst=((-(x+1)/4)*(2**XBit))-1\n",
        "\t\t\t\tNlast=((-(x/4)*(2**XBit))-1)\n",
        "\n",
        "\t\t\t\tif (WeightBuffer1D[i]>Nfirst) &  (WeightBuffer1D[i]<=Nlast):\n",
        "\t\t\t\t\t#the first value of each range\n",
        "\t\t\t\t\tWeightBuffer1D[i]=Nfirst+1\n",
        "\t\t\t\t\tbreak\n",
        "\t\telse:\n",
        "\t\t\tfor x in range(4):\n",
        "\t\t\t\tPfirst=((x/4)*(2**XBit))\n",
        "\t\t\t\tPlast=((x+1)/4)*(2**XBit)\n",
        "\n",
        "\t\t\t\tif (WeightBuffer1D[i]>=Pfirst) &  (WeightBuffer1D[i]<Plast):\n",
        "\t\t\t\t\t#the first value of each range\n",
        "\t\t\t\t\tWeightBuffer1D[i]=Pfirst\n",
        "\t\t\t\t\tbreak\n",
        "\t\ti+=1\n",
        "\tprint(\"WB-DetectTheFirstValueOfRange=\",WeightBuffer1D)\n",
        "\treturn WeightBuffer1D\n",
        "     \n",
        "#/////////////////////////////////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "def UpDownCounter(Enable,x,IncDec):\n",
        "\tIntX=int(x)\n",
        "\tEnable=int(Enable)\n",
        "\n",
        "\tif Enable != 0:\n",
        "\t\tif IntX == 1:\n",
        "\t\t\tif IncDec == 1:\n",
        "\t\t\t\tIntX=1\n",
        "\t\t\telse:\n",
        "\t\t\t\tIntX=-1\n",
        "\t\telse:\n",
        "\t\t\tIntX=0\n",
        "\telse:\n",
        "\t\tIntX=0\n",
        "\treturn IntX\n",
        "\n",
        "#/////////////////////////////////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "def DownCounter(x):\n",
        " x-=1\n",
        " return x\n",
        "\n",
        "#/////////////////////////////////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "def BISC(In):\n",
        "\t \n",
        "\tLenSc= 2 ** XBit\n",
        "\t\n",
        "\tsc=np.array([])\n",
        "\t\n",
        "\tfor x in range(LenSc):\n",
        "\t\tsc=np.append(sc,[0])\n",
        "\t\t\n",
        "\t\n",
        "\tfor x in range(XBit):\n",
        "\t\ti=(2 ** x)-1\n",
        "\t\twhile i<LenSc:\n",
        "\t\t\tsc[i] = In[x]\n",
        "\t\t\ti+= 2 ** (x+1)\n",
        "\t\t\t\n",
        "\tsc = np.flip(sc)\n",
        "\tprint(\"sc=\",sc)\n",
        "\t\n",
        "\treturn sc\n",
        "\n",
        "#/////////////////////////////////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "def DecimalToBinary(num,XBit):\n",
        "\t\n",
        "\ti = 0\n",
        "\tbnum = []\n",
        "\tBinary=''\n",
        "\twhile num!=0:\n",
        "\t\trem = num%2\n",
        "\t\tbnum.insert(i, rem)\n",
        "\t\ti = i+1\n",
        "\t\tnum = int(num/2)\n",
        "\ti = i-1\n",
        "\twhile i>=0:\n",
        "\t\tBinary=Binary+str(bnum[i])\n",
        "\t\ti = i-1\n",
        "\t\t\n",
        "\twhile len(Binary)<XBit:\n",
        "\t\tBinary='0'+Binary\n",
        "\t\t\n",
        "\tprint(\"Binary:\",Binary)\n",
        "\n",
        "\treturn Binary\n",
        "  \n",
        "#/////////////////////////////////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "def GetDefferentialWArray(w):\n",
        "\tDifferentialWArray=np.array([])\n",
        " \n",
        "\n",
        "\t#sort weight 1-D\n",
        "\tWeightSortArray1D=np.sort(w)\n",
        "\tprint(\"WeightSortArray1D=\",WeightSortArray1D)\n",
        "\t\n",
        "\n",
        "\t#create sorted weight array based on Differential of weights\n",
        "\tDifferentialWArray=np.append(DifferentialWArray,WeightSortArray1D[0])\n",
        "\tfor x in range((XWBuf*YWBuf)-1):\n",
        "\t\tDifferential=WeightSortArray1D[x+1]-WeightSortArray1D[x]\n",
        "\t\tDifferentialWArray=np.append(DifferentialWArray,Differential)\n",
        "\tprint(\"DifferentialWArray=\",DifferentialWArray)\n",
        "\treturn DifferentialWArray\n",
        "\n",
        "#/////////////////////////////////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "def SortDicIndexWeight(w):\n",
        "\n",
        "\tfor x in range(XWBuf):\n",
        "\t\tfor y in range(YWBuf):\n",
        "\t\t\tstr=x,y\n",
        "\t\t\tDicWeight[str]=w[x,y]\n",
        "\t\n",
        "\t#sort dic of weight baes on value:dic(key,value)\n",
        "\tDicWSort=dict(sorted(DicWeight.items(), key=lambda item: item[1]))\n",
        "\tprint(\"DicWSort=\",DicWSort)\n",
        "\n",
        "\tfor x in range(XWBuf*YWBuf):\n",
        "\t\tListKeyWeight.append(list(DicWSort.keys())[x])\n",
        "\tprint(\"ListKeyWeight=\",ListKeyWeight)\n",
        "\n",
        "#/////////////////////////////////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "A8yv_KxL1zfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Conv:\n",
        "    \n",
        "    def __init__(self, num_filters,WeightBuffer):\n",
        "        self.num_filters = num_filters\n",
        "        self.WeightBuffer = WeightBuffer\n",
        "\n",
        "\n",
        "    #function for doing the functional of PE in skippy based on BISC & Uo/Down counter & Down counter\n",
        "    def Mul(InputBuffer,WeightBuffer):\n",
        "    \tOutputArray=np.array([])\n",
        "    \n",
        "    \n",
        "    \tWeightBuffer1D=DetectTheFirstValueOfRange(WeightBuffer)\n",
        "    \n",
        "    \tDifferentialW=GetDefferentialWArray(WeightBuffer1D)\t\n",
        "    \n",
        "    \tSortDicIndexWeight(WeightBuffer)\n",
        "    \n",
        "    \t\n",
        "    \tfor x in range((XInputBuf-XWBuf)+1):\n",
        "    \t\tfor y in range((YInputBuf-YWBuf)+1):\n",
        "    \t\t\t\n",
        "    \t\t\t\n",
        "    \t\t\tSelectIn=InputBuffer[x:XWBuf+x ,y:YWBuf+y]\n",
        "    \t\t\t\n",
        "    \t\t\tprint(\"NewArray=\",SelectIn)\n",
        "    \t\t\t\n",
        "    \t\t\tSumOut=0\n",
        "    \t\t\tfor a in range(XWBuf):\n",
        "    \t\t\t\tfor b in range(YWBuf):\n",
        "    \t\t\t\t\t\n",
        "    \t\t\t\t\tLenDiffW= len(DifferentialW)\n",
        "    \t\t\t\t\tSumList=[]\n",
        "    \t\t\t\t\tfor s in range(LenDiffW):\n",
        "    \t\t\t\t\t\tSumList.append(0)\n",
        "    \t\t\t\t\t\t\n",
        "    \t\t\t\t\tX=SelectIn[a,b]\n",
        "    \t\t\t\t\tXBinary=DecimalToBinary(X,XBit)\n",
        "    \n",
        "    \t\t\t\t\t\n",
        "    \t\t\t\t\tSC=BISC(XBinary)\n",
        "    \n",
        "    \t\t\t\t\tandisY=0\n",
        "    \t\t\t\t\tfor z in range(LenDiffW):\n",
        "    \t\t\t\t\t\tandisSC=0\n",
        "    \t\t\t\t\t\t\n",
        "    \t\t\t\t\t\tValWInDiff=DifferentialW[z]\n",
        "    \t\t\t\t\t\t#print(type(ValWInDiff))\n",
        "            \n",
        "    \n",
        "    \n",
        "    \t\t\t\t\t\tif ValWInDiff == 0:\n",
        "    \t\t\t\t\t\t\tif z==0:\n",
        "    \t\t\t\t\t\t\t\tSumList[andisY]=0\n",
        "    \t\t\t\t\t\t\telse:\n",
        "    \t\t\t\t\t\t\t\tSumList[andisY]=SumList[andisY-1]\n",
        "    \t\t\t\t\t\t\tandisY+=1\n",
        "    \t\t\t\t\t\t\tcontinue\t\t\t\t\t\t\n",
        "    \t\t\t\t\t\t\n",
        "    \t        \t\t\t#instantiating the variable of sign holder \n",
        "    \t\t\t\t\t\t#if sign=0, number is negative & sign=1, number is positive\n",
        "    \t\t\t\t\t\t\t\t\t\t\t\t\n",
        "    \t\t\t\t\t\tif ValWInDiff<0:\n",
        "    \t\t\t\t\t\t\tsign=0\n",
        "    \t\t\t\t\t\t\tValWInDiff=abs(ValWInDiff)\n",
        "    \t\t\t\t\t\telse:\n",
        "    \t\t\t\t\t\t\tsign=1\n",
        "    \t\t\t\t\t\t\t\n",
        "    \n",
        "    \t\t\t\t\t\t\n",
        "    \t\t\t\t\t\tsum=0\n",
        "    \t\t\t\t\t\twhile ValWInDiff>0:\n",
        "    \t\t\t\t\t\t\t\n",
        "    \t\t\t\t\t\t\tOutUDCounter=UpDownCounter(ValWInDiff,SC[(len(SC)-1)-andisSC],sign)\n",
        "    \t\t\t\t\t\t\tandisSC+=1\n",
        "    \t\t\t\t\t\t\tValWInDiff=DownCounter(ValWInDiff)\n",
        "    \t\t\t\t\t\t\tsum=sum+OutUDCounter\n",
        "    \t\t\t\t\t\t\t\n",
        "    \t\t\t\t\t\tif z==0:\n",
        "    \t\t\t\t\t\t\tSumList[andisY]=sum\n",
        "    \t\t\t\t\t\telse:\n",
        "    \t\t\t\t\t\t\tSumList[andisY]=SumList[andisY-1]+sum\n",
        "    \t\t\t\t\t\tandisY+=1\n",
        "    \n",
        "    \t\t\t\t\tprint(\"SumList=\",SumList)\n",
        "    \n",
        "    \t\t\t\t\tIndex=ListKeyWeight.index((a,b))\n",
        "    \t\t\t\t\tval=SumList[Index]\t\n",
        "    \t\t\t\t\tSumOut=SumOut+val\n",
        "    \n",
        "    \t\t\t\t\n",
        "    \n",
        "    \t\t\tOutputArray=np.append(OutputArray,SumOut)\n",
        "    \t\t\t\n",
        "    \n",
        "    \tprint(\"OutputArray=\",OutputArray)\n",
        "    \tNewOutputArray=OutputArray.reshape((XInputBuf-XWBuf)+1,(YInputBuf-YWBuf)+1)\n",
        "    \tprint(\"NewOutputArray=\",NewOutputArray)\n",
        "    \treturn NewOutputArray\n",
        "\n",
        "    \n",
        "\n",
        "                \n",
        "    def forward(self, input):\n",
        "        self.last_input = input\n",
        "               \n",
        "        return Mul(input)\n",
        "    \n",
        "    def backprop(self, d_l_d_out, learn_rate):\n",
        "        '''\n",
        "        Performs a backward pass of the conv layer.\n",
        "        - d_L_d_out is the loss gradient for this layer's outputs.\n",
        "        - learn_rate is a float.\n",
        "        '''\n",
        "        d_l_d_filters = np.zeros(self.filters.shape)\n",
        "\n",
        "        for im_region, i, j in self.iterate_regions(self.last_input):\n",
        "            for f in range(self.num_filters):\n",
        "                d_l_d_filters[f] += d_l_d_out[i,j,f] * im_region\n",
        "\n",
        "        #update filters\n",
        "        self.filters -= learn_rate * d_l_d_filters\n",
        "\n",
        "        return None"
      ],
      "metadata": {
        "id": "1UIx11mSeOg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MaxPool:\n",
        "    def iterate_regions(self, image):\n",
        "        h, w, _ = image.shape\n",
        "        \n",
        "        new_h = h // 2\n",
        "        new_w = w // 2\n",
        "        \n",
        "        for i in range(new_h):\n",
        "            for j in range(new_w):\n",
        "                im_region = image[(i*2):(i*2+2), (j*2):(j*2+2)]\n",
        "                yield im_region, i, j\n",
        "                \n",
        "    def forward(self, input):\n",
        "        \n",
        "        self.last_input = input\n",
        "        \n",
        "        h, w, num_filters = input.shape\n",
        "        output = np.zeros((h//2, w//2, num_filters))\n",
        "        \n",
        "        for im_region, i, j in self.iterate_regions(input):\n",
        "            output[i,j] = np.amax(im_region,axis=(0,1))\n",
        "            \n",
        "        return output\n",
        "    \n",
        "    def backprop(self, d_l_d_out):\n",
        "        '''\n",
        "        Performs a backward pass of the maxpool layer.\n",
        "        Returns the loss gradient for this layer's inputs.\n",
        "        - d_L_d_out is the loss gradient for this layer's outputs.\n",
        "        '''\n",
        "        d_l_d_input = np.zeros(self.last_input.shape)\n",
        "\n",
        "        for im_region, i, j in self.iterate_regions(self.last_input):\n",
        "            h, w, f = im_region.shape\n",
        "            amax = np.amax(im_region, axis=(0,1))\n",
        "\n",
        "            for i2 in range(h):\n",
        "                for j2 in range(w):\n",
        "                    for f2 in range(f):\n",
        "                        #if the pixel was the max value, copy the gradient to it\n",
        "                        if(im_region[i2,j2,f2] == amax[f2]):\n",
        "                            d_l_d_input[i*2+i2, j*2+j2 ,f2] = d_l_d_out[i, j, f2]\n",
        "                            break;\n",
        "        return d_l_d_input"
      ],
      "metadata": {
        "id": "fKIn7zmXxtfn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTrTMpTwtLXd"
      },
      "source": [
        "class FCLayer:\n",
        "    def __init__(self, input_size, output_size):\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.weights = np.random.randn(input_size, output_size) / np.sqrt(input_size + output_size)\n",
        "        self.bias = np.random.randn(1, output_size) / np.sqrt(input_size + output_size)\n",
        "\n",
        "    def forward(self, input):\n",
        "        self.input = input\n",
        "        return np.dot(input, self.weights) + self.bias\n",
        "\n",
        "    def backward(self, output_error, learning_rate):\n",
        "        input_error = np.dot(output_error, self.weights.T)\n",
        "        weights_error = np.dot(self.input.T, output_error)\n",
        "        # bias_error = output_error\n",
        "        \n",
        "        self.weights -= learning_rate * weights_error\n",
        "        self.bias -= learning_rate * output_error\n",
        "        return input_error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6nSYAB2sam3"
      },
      "source": [
        "class ActivationLayer:\n",
        "    def __init__(self, activation, activation_prime):\n",
        "        self.activation = activation\n",
        "        self.activation_prime = activation_prime\n",
        "    \n",
        "    def forward(self, input):\n",
        "        self.input = input\n",
        "        return self.activation(input)\n",
        "    \n",
        "    def backward(self, output_error, learning_rate):\n",
        "        return output_error * self.activation_prime(self.input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hl8LxP1lAEiN"
      },
      "source": [
        "# bonus\n",
        "class FlattenLayer:\n",
        "    def __init__(self, input_shape):\n",
        "        self.input_shape = input_shape\n",
        "\n",
        "    def forward(self, input):\n",
        "        return np.reshape(input, (1, -1))\n",
        "    \n",
        "    def backward(self, output_error, learning_rate):\n",
        "        return np.reshape(output_error, self.input_shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQeuIfkK3vyl"
      },
      "source": [
        "# bonus\n",
        "class SoftmaxLayer:\n",
        "    def __init__(self, input_size):\n",
        "        self.input_size = input_size\n",
        "    \n",
        "    def forward(self, input):\n",
        "        self.input = input\n",
        "        tmp = np.exp(input)\n",
        "        self.output = tmp / np.sum(tmp)\n",
        "        return self.output\n",
        "    \n",
        "    def backward(self, output_error, learning_rate):\n",
        "        input_error = np.zeros(output_error.shape)\n",
        "        out = np.tile(self.output.T, self.input_size)\n",
        "        return self.output * np.dot(output_error, np.identity(self.input_size) - out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LuPbn70Wt8Q7"
      },
      "source": [
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_prime(x):\n",
        "    return np.exp(-x) / (1 + np.exp(-x))**2\n",
        "\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "def tanh_prime(x):\n",
        "    return 1 - np.tanh(x)**2\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(x, 0)\n",
        "\n",
        "def relu_prime(x):\n",
        "    return np.array(x >= 0).astype('int')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXY7jkUzuqEk"
      },
      "source": [
        "def mse(y_true, y_pred):\n",
        "    return np.mean(np.power(y_true - y_pred, 2))\n",
        "\n",
        "def mse_prime(y_true, y_pred):\n",
        "    return 2 * (y_pred - y_true) / y_pred.size\n",
        "\n",
        "def sse(y_true, y_pred):\n",
        "    return 0.5 * np.sum(np.power(y_true - y_pred, 2))\n",
        "\n",
        "def sse_prime(y_true, y_pred):\n",
        "    return y_pred - y_true"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-whGNp8Joaur",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1961b04-ed09-4862-c330-8ec3b4f1db84"
      },
      "source": [
        "from keras.datasets import mnist\n",
        "from keras.utils import np_utils\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_train /= 255\n",
        "y_train = np_utils.to_categorical(y_train)\n",
        "x_train = x_train[0:1000]\n",
        "y_train = y_train[0:1000]\n",
        "\n",
        "x_test = x_test.astype('float32')\n",
        "x_test /= 255\n",
        "y_test = np_utils.to_categorical(y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHQpwN8LpKiN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "1e705fe9-737e-4684-9efa-b5c556e67ca5"
      },
      "source": [
        "# unlike the Medium article, I am not encapsulating this process in a separate class\n",
        "# I think it is nice just like this\n",
        "network = [\n",
        "    Conv(input_shape=(28, 28)),\n",
        "    ActivationLayer(tanh, tanh_prime),\n",
        "    MaxPool(),\n",
        "    Conv(),\n",
        "    ActivationLayer(tanh, tanh_prime),\n",
        "    MaxPool(),\n",
        "    Conv(),\n",
        "    ActivationLayer(tanh, tanh_prime),\n",
        "    MaxPool(),\n",
        "    FCLayer(28 * 28, 128),\n",
        "    ActivationLayer(tanh, tanh_prime),\n",
        "    FCLayer(128, 10),\n",
        "    SoftmaxLayer(10)\n",
        "]\n",
        "\n",
        "epochs = 40\n",
        "learning_rate = 0.1\n",
        "\n",
        "# training\n",
        "for epoch in range(epochs):\n",
        "    error = 0\n",
        "    for x, y_true in zip(x_train, y_train):\n",
        "        # forward\n",
        "        output = x\n",
        "        for layer in network:\n",
        "            output = layer.forward(output)\n",
        "        \n",
        "        # error (display purpose only)\n",
        "        error += mse(y_true, output)\n",
        "\n",
        "        # backward\n",
        "        output_error = mse_prime(y_true, output)\n",
        "        for layer in reversed(network):\n",
        "            output_error = layer.backward(output_error, learning_rate)\n",
        "    \n",
        "    error /= len(x_train)\n",
        "    print('%d/%d, error=%f' % (epoch + 1, epochs, error))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-8f6035565a64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# I think it is nice just like this\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m network = [\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mConv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mActivationLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtanh_prime\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mMaxPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'input_shape'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrMLx3eGv3jk"
      },
      "source": [
        "def predict(network, input):\n",
        "    output = input\n",
        "    for layer in network:\n",
        "        output = layer.forward(output)\n",
        "    return output\n",
        "\n",
        "ratio = sum([np.argmax(y) == np.argmax(predict(network, x)) for x, y in zip(x_test, y_test)]) / len(x_test)\n",
        "error = sum([mse(y, predict(network, x)) for x, y in zip(x_test, y_test)]) / len(x_test)\n",
        "print('ratio: %.2f' % ratio)\n",
        "print('mse: %.4f' % error)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERV5_QinvwXY"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "samples = 10\n",
        "for test, true in zip(x_test[:samples], y_test[:samples]):\n",
        "    image = np.reshape(test, (28, 28))\n",
        "    plt.imshow(image, cmap='binary')\n",
        "    plt.show()\n",
        "    pred = predict(network, test)[0]\n",
        "    idx = np.argmax(pred)\n",
        "    idx_true = np.argmax(true)\n",
        "    print('pred: %s, prob: %.2f, true: %d' % (idx, pred[idx], idx_true))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}