{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO4y9/PTktQmadk0SLqEMis",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fatisepah/samples/blob/CNN-from-scratch/implementation_lenet_from_scratch_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import functools\n",
        "import operator\n",
        "import gzip\n",
        "import struct\n",
        "import array\n",
        "import numpy as np\n",
        "from urllib import urlretrieve\n",
        "import sys\n",
        "\n",
        "#from: https://github.com/datapythonista/mnist\n",
        "DATASET_DIRECTORY = 'data/'\n",
        "URL = 'http://yann.lecun.com/exdb/mnist/'\n",
        "\n",
        "def parse_idx(fd):\n",
        "    DATA_TYPES = {0x08: 'B',  # unsigned byte\n",
        "                  0x09: 'b',  # signed byte\n",
        "                  0x0b: 'h',  # short (2 bytes)\n",
        "                  0x0c: 'i',  # int (4 bytes)\n",
        "                  0x0d: 'f',  # float (4 bytes)\n",
        "                  0x0e: 'd'}  # double (8 bytes)\n",
        "\n",
        "    header = fd.read(4)\n",
        "    if len(header) != 4:\n",
        "        raise IdxDecodeError('Invalid IDX file, file empty or does not contain a full header.')\n",
        "\n",
        "    zeros, data_type, num_dimensions = struct.unpack('>HBB', header)\n",
        "\n",
        "    if zeros != 0:\n",
        "        raise IdxDecodeError('Invalid IDX file, file must start with two zero bytes. '\n",
        "                             'Found 0x%02x' % zeros)\n",
        "\n",
        "    try:\n",
        "        data_type = DATA_TYPES[data_type]\n",
        "    except KeyError:\n",
        "        raise IdxDecodeError('Unknown data type 0x%02x in IDX file' % data_type)\n",
        "\n",
        "    dimension_sizes = struct.unpack('>' + 'I' * num_dimensions,\n",
        "                                    fd.read(4 * num_dimensions))\n",
        "\n",
        "    data = array.array(data_type, fd.read())\n",
        "    data.byteswap()  # looks like array.array reads data as little endian\n",
        "\n",
        "    expected_items = functools.reduce(operator.mul, dimension_sizes)\n",
        "    if len(data) != expected_items:\n",
        "        raise IdxDecodeError('IDX file has wrong number of items. '\n",
        "                             'Expected: %d. Found: %d' % (expected_items, len(data)))\n",
        "\n",
        "    return np.array(data).reshape(dimension_sizes)\n",
        "\n",
        "def print_download_progress(count, block_size, total_size):\n",
        "    pct_complete = int(count * block_size * 100 / total_size)\n",
        "    pct_complete = min(pct_complete, 100)\n",
        "    msg = \"\\r- Download progress: %d\" % (pct_complete) + \"%\"\n",
        "    sys.stdout.write(msg)\n",
        "    sys.stdout.flush()\n",
        "\n",
        "\n",
        "def download_and_parse_mnist_file(fname, target_dir=None, force=False):\n",
        "    if not os.path.exists(DATASET_DIRECTORY):\n",
        "        os.makedirs(DATASET_DIRECTORY)\n",
        "    if not os.path.exists(DATASET_DIRECTORY+fname):\n",
        "        print('Downloading '+fname)\n",
        "        file_path = os.path.join(DATASET_DIRECTORY, fname)\n",
        "        url = URL + fname\n",
        "        file_path, _ = urlretrieve(url=url, filename=file_path, reporthook=print_download_progress)\n",
        "        print(\"\\nDownload finished.\")\n",
        "\n",
        "    fname = 'data/' + fname\n",
        "    fopen = gzip.open if os.path.splitext(fname)[1] == '.gz' else open\n",
        "    with fopen(fname, 'rb') as fd:\n",
        "        return parse_idx(fd)\n",
        "\n",
        "\n",
        "def train_images():\n",
        "    return download_and_parse_mnist_file('train-images-idx3-ubyte.gz')\n",
        "\n",
        "\n",
        "def test_images():\n",
        "    return download_and_parse_mnist_file('t10k-images-idx3-ubyte.gz')\n",
        "\n",
        "\n",
        "def train_labels():\n",
        "    return download_and_parse_mnist_file('train-labels-idx1-ubyte.gz')\n",
        "\n",
        "\n",
        "def test_labels():\n",
        "    return download_and_parse_mnist_file('t10k-labels-idx1-ubyte.gz')"
      ],
      "metadata": {
        "id": "KFvf18GBLjdH",
        "outputId": "035f426a-99d7-4f4f-933c-ca8be456e856",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-64eb483fea7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0murllib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0murlretrieve\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'urlretrieve' from 'urllib' (/usr/lib/python3.7/urllib/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#https://github.com/zishansami102/CNN-from-Scratch/blob/master/preprocessing.py\n",
        "import numpy as np\n",
        "import cv2\n",
        "from scipy import ndimage\n",
        "import math\n",
        "\n",
        "\n",
        "def shift(img,sx,sy):\n",
        "    rows,cols = img.shape\n",
        "    M = np.float32([[1,0,sx],[0,1,sy]])\n",
        "    shifted = cv2.warpAffine(img,M,(cols,rows))\n",
        "    return shifted\n",
        "\n",
        "def getBestShift(img):\n",
        "    cy,cx = ndimage.measurements.center_of_mass(img)\n",
        "\n",
        "    rows,cols = img.shape\n",
        "    shiftx = np.round(cols/2.0-cx).astype(int)\n",
        "    shifty = np.round(rows/2.0-cy).astype(int)\n",
        "\n",
        "    return shiftx,shifty\n",
        "\n",
        "def preprocess(img):\n",
        "    img=255-np.array(img).reshape(28,28).astype(np.uint8)\n",
        "    (thresh, gray) = cv2.threshold(img, 128, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n",
        "\n",
        "    while np.sum(gray[0]) == 0:\n",
        "        gray = gray[1:]\n",
        "\n",
        "    while np.sum(gray[:,0]) == 0:\n",
        "        gray = np.delete(gray,0,1)\n",
        "\n",
        "    while np.sum(gray[-1]) == 0:\n",
        "        gray = gray[:-1]\n",
        "\n",
        "    while np.sum(gray[:,-1]) == 0:\n",
        "        gray = np.delete(gray,-1,1)\n",
        "\n",
        "    rows,cols = gray.shape\n",
        "\n",
        "    if rows > cols:\n",
        "        factor = 20.0/rows\n",
        "        rows = 20\n",
        "        cols = int(round(cols*factor))\n",
        "        gray = cv2.resize(gray, (cols,rows))\n",
        "    else:\n",
        "        factor = 20.0/cols\n",
        "        cols = 20\n",
        "        rows = int(round(rows*factor))\n",
        "        gray = cv2.resize(gray, (cols, rows))\n",
        "\n",
        "    colsPadding = (int(math.ceil((28-cols)/2.0)),int(math.floor((28-cols)/2.0)))\n",
        "    rowsPadding = (int(math.ceil((28-rows)/2.0)),int(math.floor((28-rows)/2.0)))\n",
        "    gray = np.lib.pad(gray,(rowsPadding,colsPadding),'constant')\n",
        "\n",
        "    shiftx,shifty = getBestShift(gray)\n",
        "    shifted = shift(gray,shiftx,shifty)\n",
        "    gray = shifted\n",
        "\n",
        "    img = gray.reshape(1,28,28).astype(np.float32)\n",
        "\n",
        "    img-= int(33.3952)\n",
        "    img/= int(78.6662)\n",
        "    return img"
      ],
      "metadata": {
        "id": "XmGZ7HHfLBEv"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "9BsFPks4k3Wf"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "\n",
        "class Convolution2D:\n",
        "\n",
        "    def __init__(self, inputs_channel, num_filters, kernel_size, padding, stride, learning_rate, name):\n",
        "        # weight size: (F, C, K, K)\n",
        "        # bias size: (F) \n",
        "        self.F = num_filters\n",
        "        self.K = kernel_size\n",
        "        self.C = inputs_channel\n",
        "\n",
        "        self.weights = np.zeros((self.F, self.C, self.K, self.K))\n",
        "        self.bias = np.zeros((self.F, 1))\n",
        "        for i in range(0,self.F):\n",
        "            self.weights[i,:,:,:] = np.random.normal(loc=0, scale=np.sqrt(1./(self.C*self.K*self.K)), size=(self.C, self.K, self.K))\n",
        "\n",
        "        self.p = padding\n",
        "        self.s = stride\n",
        "        self.lr = learning_rate\n",
        "        self.name = name\n",
        "\n",
        "    def zero_padding(self, inputs, size):\n",
        "        w, h = inputs.shape[0], inputs.shape[1]\n",
        "        new_w = 2 * size + w\n",
        "        new_h = 2 * size + h\n",
        "        out = np.zeros((new_w, new_h))\n",
        "        out[size:w+size, size:h+size] = inputs\n",
        "        return out\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # input size: (C, W, H)\n",
        "        # output size: (N, F ,WW, HH)\n",
        "        C = inputs.shape[0]\n",
        "        W = inputs.shape[1]+2*self.p\n",
        "        H = inputs.shape[2]+2*self.p\n",
        "        self.inputs = np.zeros((C, W, H))\n",
        "        for c in range(inputs.shape[0]):\n",
        "            self.inputs[c,:,:] = self.zero_padding(inputs[c,:,:], self.p)\n",
        "        WW = (W - self.K)/self.s + 1\n",
        "        HH = (H - self.K)/self.s + 1\n",
        "        feature_maps = np.zeros((self.F, WW, HH))\n",
        "        for f in range(self.F):\n",
        "            for w in range(0, WW, self.s):\n",
        "                for h in range(0, HH, self.s):\n",
        "                    feature_maps[f,w,h]=np.sum(self.inputs[:,w:w+self.K,h:h+self.K]*self.weights[f,:,:,:])+self.bias[f]\n",
        "\n",
        "        return feature_maps\n",
        "\n",
        "    def backward(self, dy):\n",
        "\n",
        "        C, W, H = self.inputs.shape\n",
        "        dx = np.zeros(self.inputs.shape)\n",
        "        dw = np.zeros(self.weights.shape)\n",
        "        db = np.zeros(self.bias.shape)\n",
        "\n",
        "        F, W, H = dy.shape\n",
        "        for f in range(F):\n",
        "            for w in range(0, W, self.s):\n",
        "                for h in range(0, H, self.s):\n",
        "                    dw[f,:,:,:]+=dy[f,w,h]*self.inputs[:,w:w+self.K,h:h+self.K]\n",
        "                    dx[:,w:w+self.K,h:h+self.K]+=dy[f,w,h]*self.weights[f,:,:,:]\n",
        "\n",
        "        for f in range(F):\n",
        "            db[f] = np.sum(dy[f, :, :])\n",
        "\n",
        "        self.weights -= self.lr * dw\n",
        "        self.bias -= self.lr * db\n",
        "        return dx\n",
        "\n",
        "    def extract(self):\n",
        "        return {self.name+'.weights':self.weights, self.name+'.bias':self.bias}\n",
        "\n",
        "    def feed(self, weights, bias):\n",
        "        self.weights = weights\n",
        "        self.bias = bias\n",
        "\n",
        "class Maxpooling2D:\n",
        "\n",
        "    def __init__(self, pool_size, stride, name):\n",
        "        self.pool = pool_size\n",
        "        self.s = stride\n",
        "        self.name = name\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        self.inputs = inputs\n",
        "        C, W, H = inputs.shape\n",
        "        new_width = (W - self.pool)/self.s + 1\n",
        "        new_height = (H - self.pool)/self.s + 1\n",
        "        out = np.zeros((C, new_width, new_height))\n",
        "        for c in range(C):\n",
        "            for w in range(W/self.s):\n",
        "                for h in range(H/self.s):\n",
        "                    out[c, w, h] = np.max(self.inputs[c, w*self.s:w*self.s+self.pool, h*self.s:h*self.s+self.pool])\n",
        "        return out\n",
        "\n",
        "    def backward(self, dy):\n",
        "        C, W, H = self.inputs.shape\n",
        "        dx = np.zeros(self.inputs.shape)\n",
        "        \n",
        "        for c in range(C):\n",
        "            for w in range(0, W, self.pool):\n",
        "                for h in range(0, H, self.pool):\n",
        "                    st = np.argmax(self.inputs[c,w:w+self.pool,h:h+self.pool])\n",
        "                    (idx, idy) = np.unravel_index(st, (self.pool, self.pool))\n",
        "                    dx[c, w+idx, h+idy] = dy[c, w/self.pool, h/self.pool]\n",
        "        return dx\n",
        "\n",
        "    def extract(self):\n",
        "        return \n",
        "    \n",
        "class FullyConnected:\n",
        "\n",
        "    def __init__(self, num_inputs, num_outputs, learning_rate, name):\n",
        "        self.weights = 0.01*np.random.rand(num_inputs, num_outputs)\n",
        "        self.bias = np.zeros((num_outputs, 1))\n",
        "        self.lr = learning_rate\n",
        "        self.name = name\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        self.inputs = inputs\n",
        "        return np.dot(self.inputs, self.weights) + self.bias.T\n",
        "\n",
        "    def backward(self, dy):\n",
        "\n",
        "        if dy.shape[0] == self.inputs.shape[0]:\n",
        "            dy = dy.T\n",
        "        dw = dy.dot(self.inputs)\n",
        "        db = np.sum(dy, axis=1, keepdims=True)\n",
        "        dx = np.dot(dy.T, self.weights.T)\n",
        "\n",
        "        self.weights -= self.lr * dw.T\n",
        "        self.bias -= self.lr * db\n",
        "\n",
        "        return dx\n",
        "\n",
        "    def extract(self):\n",
        "        return {self.name+'.weights':self.weights, self.name+'.bias':self.bias}\n",
        "\n",
        "    def feed(self, weights, bias):\n",
        "        self.weights = weights\n",
        "        self.bias = bias\n",
        "\n",
        "class Flatten:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    def forward(self, inputs):\n",
        "        self.C, self.W, self.H = inputs.shape\n",
        "        return inputs.reshape(1, self.C*self.W*self.H)\n",
        "    def backward(self, dy):\n",
        "        return dy.reshape(self.C, self.W, self.H)\n",
        "    def extract(self):\n",
        "        return\n",
        "\n",
        "class ReLu:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    def forward(self, inputs):\n",
        "        self.inputs = inputs\n",
        "        ret = inputs.copy()\n",
        "        ret[ret < 0] = 0\n",
        "        return ret\n",
        "    def backward(self, dy):\n",
        "        dx = dy.copy()\n",
        "        dx[self.inputs < 0] = 0\n",
        "        return dx\n",
        "    def extract(self):\n",
        "        return\n",
        "\n",
        "class Softmax:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    def forward(self, inputs):\n",
        "        exp = np.exp(inputs, dtype=np.float)\n",
        "        self.out = exp/np.sum(exp)\n",
        "        return self.out\n",
        "    def backward(self, dy):\n",
        "        return self.out.T - dy.reshape(dy.shape[0],1)\n",
        "    def extract(self):\n",
        "        return\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# loss\n",
        "def cross_entropy(inputs, labels):\n",
        "\n",
        "    out_num = labels.shape[0]\n",
        "    p = np.sum(labels.reshape(1,out_num)*inputs)\n",
        "    loss = -np.log(p)\n",
        "    return loss"
      ],
      "metadata": {
        "id": "BIbbJDGclWqZ"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pickle \n",
        "import sys\n",
        "from time import *\n",
        "#from model.loss import *\n",
        "#from model.layers import *\n",
        "\n",
        "class Net:\n",
        "    def __init__(self):\n",
        "        # Lenet\n",
        "        # input: 28x28\n",
        "        # conv1: (5x5x6)@s1p2 -> 28x28x6 {(28-5+2x2)/1+1}\n",
        "        # maxpool2: (2x2)@s2 -> 14x14x6 {(28-2)/2+1}\n",
        "        # conv3: (5x5x16)@s1p0 -> 10x10x16 {(14-5)/1+1}\n",
        "        # maxpool4: (2x2)@s2 -> 5x5x16 {(10-2)/2+1}\n",
        "        # conv5: (5x5x120)@s1p0 -> 1x1x120 {(5-5)/1+1}\n",
        "        # fc6: 120 -> 84\n",
        "        # fc7: 84 -> 10\n",
        "        # softmax: 10 -> 10\n",
        "        lr = 0.01\n",
        "        self.layers = []\n",
        "        self.layers.append(Convolution2D(inputs_channel=1, num_filters=6, kernel_size=5, padding=2, stride=1, learning_rate=lr, name='conv1'))\n",
        "        self.layers.append(ReLu())\n",
        "        self.layers.append(Maxpooling2D(pool_size=2, stride=2, name='maxpool2'))\n",
        "        self.layers.append(Convolution2D(inputs_channel=6, num_filters=16, kernel_size=5, padding=0, stride=1, learning_rate=lr, name='conv3'))\n",
        "        self.layers.append(ReLu())\n",
        "        self.layers.append(Maxpooling2D(pool_size=2, stride=2, name='maxpool4'))\n",
        "        self.layers.append(Convolution2D(inputs_channel=16, num_filters=120, kernel_size=5, padding=0, stride=1, learning_rate=lr, name='conv5'))\n",
        "        self.layers.append(ReLu())\n",
        "        self.layers.append(Flatten())\n",
        "        self.layers.append(FullyConnected(num_inputs=120, num_outputs=84, learning_rate=lr, name='fc6'))\n",
        "        self.layers.append(ReLu())\n",
        "        self.layers.append(FullyConnected(num_inputs=84, num_outputs=10, learning_rate=lr, name='fc7'))\n",
        "        self.layers.append(Softmax())\n",
        "        self.lay_num = len(self.layers)\n",
        "\n",
        "    def train(self, training_data, training_label, batch_size, epoch, weights_file):\n",
        "        total_acc = 0\n",
        "        for e in range(epoch):\n",
        "            for batch_index in range(0, training_data.shape[0], batch_size):\n",
        "                # batch input\n",
        "                if batch_index + batch_size < training_data.shape[0]:\n",
        "                    data = training_data[batch_index:batch_index+batch_size]\n",
        "                    label = training_label[batch_index:batch_index + batch_size]\n",
        "                else:\n",
        "                    data = training_data[batch_index:training_data.shape[0]]\n",
        "                    label = training_label[batch_index:training_label.shape[0]]\n",
        "                loss = 0\n",
        "                acc = 0\n",
        "                start_time = time()\n",
        "                for b in range(batch_size):\n",
        "                    x = data[b]\n",
        "                    y = label[b]\n",
        "                    # forward pass\n",
        "                    for l in range(self.lay_num):\n",
        "                        output = self.layers[l].forward(x)\n",
        "                        x = output\n",
        "                    loss += cross_entropy(output, y)\n",
        "                    if np.argmax(output) == np.argmax(y):\n",
        "                        acc += 1\n",
        "                        total_acc += 1\n",
        "                    # backward pass\n",
        "                    dy = y\n",
        "                    for l in range(self.lay_num-1, -1, -1):\n",
        "                        dout = self.layers[l].backward(dy)\n",
        "                        dy = dout\n",
        "                # time\n",
        "                end_time = time()\n",
        "                batch_time = end_time-start_time\n",
        "                remain_time = (training_data.shape[0]*epoch-batch_index-training_data.shape[0]*e)/batch_size*batch_time\n",
        "                hrs = int(remain_time)/3600\n",
        "                mins = int((remain_time/60-hrs*60))\n",
        "                secs = int(remain_time-mins*60-hrs*3600)\n",
        "                # result\n",
        "                loss /= batch_size\n",
        "                batch_acc = float(acc)/float(batch_size)\n",
        "                training_acc = float(total_acc)/float((batch_index+batch_size)*(e+1))\n",
        "                print('=== Epoch: {0:d}/{1:d} === Iter:{2:d} === Loss: {3:.2f} === BAcc: {4:.2f} === TAcc: {5:.2f} === Remain: {6:d} Hrs {7:d} Mins {8:d} Secs ==='.format(e,epoch,batch_index+batch_size,loss,batch_acc,training_acc,int(hrs),int(mins),int(secs)))\n",
        "        # dump weights and bias\n",
        "        obj = []\n",
        "        for i in range(self.lay_num):\n",
        "            cache = self.layers[i].extract()\n",
        "            obj.append(cache)\n",
        "        with open(weights_file, 'wb') as handle:\n",
        "            pickle.dump(obj, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "\n",
        "    def test(self, data, label, test_size):\n",
        "        toolbar_width = 40\n",
        "        sys.stdout.write(\"[%s]\" % (\" \" * (toolbar_width-1)))\n",
        "        sys.stdout.flush()\n",
        "        sys.stdout.write(\"\\b\" * (toolbar_width))\n",
        "        step = float(test_size)/float(toolbar_width)\n",
        "        st = 1\n",
        "        total_acc = 0\n",
        "        for i in range(test_size):\n",
        "            if i == round(step):\n",
        "                step += float(test_size)/float(toolbar_width)\n",
        "                st += 1\n",
        "                sys.stdout.write(\".\")\n",
        "                #sys.stdout.write(\"%s]a\"%(\" \"*(toolbar_width-st)))\n",
        "                #sys.stdout.write(\"\\b\" * (toolbar_width-st+2))\n",
        "                sys.stdout.flush()\n",
        "            x = data[i]\n",
        "            y = label[i]\n",
        "            for l in range(self.lay_num):\n",
        "                output = self.layers[l].forward(x)\n",
        "                x = output\n",
        "            if np.argmax(output) == np.argmax(y):\n",
        "                total_acc += 1\n",
        "        sys.stdout.write(\"\\n\")\n",
        "        print('=== Test Size:{0:d} === Test Acc:{1:.2f} ==='.format(test_size, float(total_acc)/float(test_size)))\n",
        "\n",
        "\n",
        "    def test_with_pretrained_weights(self, data, label, test_size, weights_file):\n",
        "        with open(weights_file, 'rb') as handle:\n",
        "            b = pickle.load(handle)\n",
        "        self.layers[0].feed(b[0]['conv1.weights'], b[0]['conv1.bias'])\n",
        "        self.layers[3].feed(b[3]['conv3.weights'], b[3]['conv3.bias'])\n",
        "        self.layers[6].feed(b[6]['conv5.weights'], b[6]['conv5.bias'])\n",
        "        self.layers[9].feed(b[9]['fc6.weights'], b[9]['fc6.bias'])\n",
        "        self.layers[11].feed(b[11]['fc7.weights'], b[11]['fc7.bias'])\n",
        "        toolbar_width = 40\n",
        "        sys.stdout.write(\"[%s]\" % (\" \" * (toolbar_width-1)))\n",
        "        sys.stdout.flush()\n",
        "        sys.stdout.write(\"\\b\" * (toolbar_width))\n",
        "        step = float(test_size)/float(toolbar_width)\n",
        "        st = 1\n",
        "        total_acc = 0\n",
        "        for i in range(test_size):\n",
        "            if i == round(step):\n",
        "                step += float(test_size)/float(toolbar_width)\n",
        "                st += 1\n",
        "                sys.stdout.write(\".\")\n",
        "                #sys.stdout.write(\"%s]a\"%(\" \"*(toolbar_width-st)))\n",
        "                #sys.stdout.write(\"\\b\" * (toolbar_width-st+2))\n",
        "                sys.stdout.flush()\n",
        "            x = data[i]\n",
        "            y = label[i]\n",
        "            for l in range(self.lay_num):\n",
        "                output = self.layers[l].forward(x)\n",
        "                x = output\n",
        "            if np.argmax(output) == np.argmax(y):\n",
        "                total_acc += 1\n",
        "        sys.stdout.write(\"\\n\")\n",
        "        print('=== Test Size:{0:d} === Test Acc:{1:.2f} ==='.format(test_size, float(total_acc)/float(test_size)))\n",
        "        \n",
        "    def predict_with_pretrained_weights(self, inputs, weights_file):\n",
        "        with open(weights_file, 'rb') as handle:\n",
        "            b = pickle.load(handle)\n",
        "        self.layers[0].feed(b[0]['conv1.weights'], b[0]['conv1.bias'])\n",
        "        self.layers[3].feed(b[3]['conv3.weights'], b[3]['conv3.bias'])\n",
        "        self.layers[6].feed(b[6]['conv5.weights'], b[6]['conv5.bias'])\n",
        "        self.layers[9].feed(b[9]['fc6.weights'], b[9]['fc6.bias'])\n",
        "        self.layers[11].feed(b[11]['fc7.weights'], b[11]['fc7.bias'])\n",
        "        for l in range(self.lay_num):\n",
        "            output = self.layers[l].forward(inputs)\n",
        "            inputs = output\n",
        "        digit = np.argmax(output)\n",
        "        probability = output[0, digit]\n",
        "        return digit, probability\n",
        "\n",
        "\n",
        "       \n"
      ],
      "metadata": {
        "id": "1w4iD4vIledK"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from model.network import Net\n",
        "from flask import Flask, jsonify, render_template, request\n",
        "from preprocessing import *\n",
        "\n",
        "app = Flask(__name__)\n",
        "@app.route('/')\n",
        "def index():\n",
        "    return render_template('index.html')\n",
        "@app.route('/digit_prediction', methods=['POST'])\n",
        "\n",
        "def digit_prediction():\n",
        "    if(request.method == \"POST\"):\n",
        "        img = request.get_json()\n",
        "        img = preprocess(img)\n",
        "        net = Net()\n",
        "        digit, probability = net.predict_with_pretrained_weights(img, 'pretrained_weights.pkl')\n",
        "        data = { \"digit\":digit, \"probability\":float(int(probability*100))/100. }\n",
        "        return jsonify(data)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    app.run(debug=True)"
      ],
      "metadata": {
        "id": "0DcvJ3KkL7J_",
        "outputId": "7e9e8987-76ce-443b-d778-2ff9c802f1ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Serving Flask app \"__main__\" (lazy loading)\n",
            " * Environment: production\n",
            "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
            "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
            " * Debug mode: on\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug: * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n",
            "INFO:werkzeug: * Restarting with stat\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:3334: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import mnist\n",
        "from keras.utils import np_utils\n",
        "import numpy as np\n",
        "import mnist\n",
        "from model.network import Net\n",
        "\n",
        "print('Loadind data......')\n",
        "num_classes = 10\n",
        "train_images = mnist.train_images() #[60000, 28, 28]\n",
        "train_labels = mnist.train_labels()\n",
        "test_images = mnist.test_images()\n",
        "test_labels = mnist.test_labels()\n",
        "\n",
        "print('Preparing data......')\n",
        "train_images -= int(np.mean(train_images))\n",
        "train_images /= int(np.std(train_images))\n",
        "test_images -= int(np.mean(test_images))\n",
        "test_images /= int(np.std(test_images))\n",
        "training_data = train_images.reshape(60000, 1, 28, 28)\n",
        "training_labels = np.eye(num_classes)[train_labels]\n",
        "testing_data = test_images.reshape(10000, 1, 28, 28)\n",
        "testing_labels = np.eye(num_classes)[test_labels]\n",
        "\n",
        "net = Net()\n",
        "print('Training Lenet......')\n",
        "net.train(training_data, training_labels, 32, 1, 'weights.pkl')\n",
        "print('Testing Lenet......')\n",
        "net.test(testing_data, testing_labels, 100)\n",
        "print('Testing with pretrained weights......')\n",
        "net.test_with_pretrained_weights(testing_data, testing_labels, 100, 'pretrained_weights.pkl')"
      ],
      "metadata": {
        "id": "-kHHJRiLiQYV",
        "outputId": "dc25750a-1f18-4a63-ced0-b3c3eb96b4ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-7fabadc3a964>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnp_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mnist'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ]
}